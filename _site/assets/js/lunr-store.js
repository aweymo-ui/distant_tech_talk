var store = [ 
    
    
    { 
        "url": "/content/1_background.html",
        "title": "Background",
        "text": "Hello, my name is Andrew Weymouth and I have worked with the University of Idaho Library as the Digital Initiatives Librarian in the Center for Digital Inquiry and Learning (CDIL) department since the fall of 2023. My work generally consists of creating and maintaining our digital collections, working with CDIL fellows, helping to rethink processes and introducing new digital scholarship tools to the department. &#10042; CollectionBuilder browse site and CollectionBuilder template interface. The University of Idaho’s Digital Scholarship and Open Strategies (DSOS) department was established in 2008 to digitize the newly acquired International Jazz Collection and has since expanded to over 130 digital collections.[1] These collections are built with CollectionBuilder, an “open source framework for creating digital collections and exhibit websites that are driven by metadata and modern static web technology”. A companion framework named Oral History as Data (OHD) was developed afterwards to visualize encoded transcriptions and allow researchers to explore oral history recordings by keywords and tags. In this presentation, “tagging” refers to a custom set of subject designations that can be tailored by the transcriber depending on the recording’s content and themes. Oral History as Data tagging interface with tags and how they are visualized within two recordings below. Our physical workspace at the library is the Center for Digital Inquiry and Learning (CDIL), where our Digital Labs Manager, Digital Project Manager and I support the labor of a small group of student workers and fellowship recipients, generally around 2-5 a semester. Both the CollectionBuilder and OHD frameworks have been designed to be completely open source, only requiring someone with access to Google Sheets, a text editor, a GitHub account and minimal software installation to create, maintain and export digital collections. &#10042; The incentive for this project arose from realizing a number of oral history recordings were either untranscribed, partially transcribed or lacking in accuracy following a data migration of our digital collections away from ContentDM in the winter of 2023. Because of the volume of transcripts that needed updating, it was worthwhile to rethink workflows for overall efficiency and accuracy of this project. This case study details my experience over the last six months developing the tools independently and collaborating with one undergraduate fellowship recipient and one graduate student working as transcribers, incorporating their feedback and streamlining processes. Additionally, this work supports an improved and expanded version of the OHD framework and documentation that is currently in progress."
    },
    { 
        "url": "/content/2_challenges.html",
        "title": "Challenges",
        "text": "The time-intensive nature of transcription has made many oral history collections an undervalued format in digital initiatives. Meeting accessibility standards involves not only transcribing recordings but also presenting them in an intuitive, keyboard navigable digital interface. OHD developer Devin Becker’s solution in the Oral History as Data (OHD) template displays the audio at the top of the page, followed by a visualization of the entire recording displaying the colored tags, a key to the tags, a search bar for keyword queries and the transcription below. This allows researchers to follow along with the timestamped transcript as the audio plays. Oral History as Data tagging interface and keyword searching functionality. Despite this advancement, the initial transcription process has remained a significant hurdle. Since OHD’s development in 2016, machine learning speech to text capabilities have improved considerably. Earlier free options were either so poor that they were negligible to working from scratch, while other options were prohibitively expensive. Completely human driven transcription has its own challenges: it’s tedious, slow moving work that isn’t going to be the highlight of a student worker’s CV, and, without close supervision, can result in lapses in quality not dissimilar to poor machine learning. &#10042; Similar lapses in identification and vulnerability to bias were also at play when student workers created tags for OHD transcriptions. Tags—ranging from locations, people, or even abstract emotions—were created by student workers as they identified them throughout the transcription process. This approach led to multiple challenges: an uncontrolled vocabulary, knowledge gaps amongst student workers, who may lack the historical, scientific or regional knowledge to identify the tags within dialogue and tags that are not apparent if transcribing a collection of recordings linearly. Challenges of Linear Listening Visualization Linear listening, creating tags by listening to an oral history collection from beginning to end, may mislead transcribers by establishing repeating themes throughout an early recording that doesn’t occur across the collection and missing themes that only begin to appear in later recordings. The name of this presentation, distant listening, is an alternate approach which text mines transcriptions and generate tags before the student worker copy editing process with the goal of producing richer, more accurate tagging, ultimately allowing researchers to more easily identify connections across entire oral history collections. &#10042;"
    },
    { 
        "url": "/content/3_process.html",
        "title": "Process",
        "text": "Contents: Overview | Premiere | Python Text Mining | Primary Tag Sheet | Formatting | Apps Script | Copyediting Workflow Visualization Overview To summarize the process before going into detail: Audio files are transcribed into CSV files be Premiere These CSVs are made into individual Google Sheets and also added to the Python Transcription Mining Tool Within the tool, these items are concatenated and this combined document is then searched for all of the associated words and phrases of each of the subjects built into the tool The tool provides a tally of the identified associated words and subjects identified across all transcriptions This data is used to create the “Primary Tag Sheet” in another Google Sheet Using the Apps Script function, all individual transcripts are linked to the primary tag sheet so their tag fields are automatically filled with the subject head on the line of dialogue with the identified associated words and phrases This way, new categories or associated words can be added or removed to the Primary Tag Sheet and these changes can be implemented across all individual transcripts by simply running the Apps Script Individual changes can be implemented during the student worker led copy editing process to catch any data driven errors &#10042; Premiere Drawing from my previous experience working oral history recordings for a digital encyclopedia, I tested Adobe Premiere’s transcription tools and found it uniquely well-suited for the Oral History as Data framework. Advantages include: Dramatically increased accuracy in differentiating speakers and transcribing dialogue, even with obscure and regional proper nouns. Significantly faster transcription speed, from one 1.5-hour recording every two to three business days up to twenty 1.5-hour recordings in one day. Costs covered by our university-wide Adobe subscription. Direct export to CSV UTF-8 needed for OHD, avoiding conversion errors. Free non-English language packs, enabling the creation of Spanish and French language oral history collections. High privacy standards with Premiere’s GDPR compliance, ensuring all transcription material is stored locally and not uploaded to the cloud.[2] Example of transcript CSV formatting as it exports from Adobe Premiere That said, the tool is not a panacea. While modern recordings in good conditions have extremely high transcription accuracy, poor quality recordings or interviews between two similar sounding people can require significant copyediting. Recent work by the Matt Miller of the Library of Congress has me very interested in creating custom speech to text tools using Whisper(.cpp) to possibly help improve on these inaccuracies.[3] To veer this tech talk into theory for a moment, while some negative perspectives of speech to text tools have to do with bias built into machine learning (Link, 2020), others stem from academic double standards expecting written transcripts to be an improved version of the audio rather than a reflection of it. Some of these notions may originate from the American roots of oral history transcription at the Oral History Research Office of Columbia University in 1948, where editors were encouraged to delete “false starts”, audit wording, rearrange passages into topical or chronological order or delete whole sections to transform the transcript from “what might be dismissed as hearsay into a document that has much the standing of legal disposition”, essentially divorcing the transcript from the audio.(Freund, 2024) Since then, critics of this practice of “cleaning up” spoken language have emerged, pointing out how it introduces unnecessary editorial bias. As University of Kentucky’s Susan Emily Allen notes in Resisting the Editorial Ego: Editing Oral History: These texts take it upon themselves to glean \"what words are meaningful.\" Meaningful for whom? For the editor? Such subjectivism is not only rather irresponsible scholarship but, however well-intentioned, an attempt to legislate truth. (Allen, 1982) &#10042; Python Text Mining After initial tests using the web based text mining tool Voyant while developing the Taylor Wilderness Research Station digital collection, I wanted to create a text mining tool from scratch using Python that would allow me to identify specific words and phrases, create custom tagging categories and “stopwords”(words removed from text before processing and analysis) for each collection. Once the CSVs of the transcript are added to a folder in the Python workspace, the code begins with importing Pandas library for data manipulation, the Natural Language Toolkit and TextBlob for language processing and sentiment analysis. Additionally, Regular Expressions and the ‘collections.Counter’ function are added for text mining and tallying results. Next, the ‘preprocess_text’ function removes words of four characters or fewer, eliminates punctuation, and converts all text to lowercase. CSV file paths are constructed, and the text data is concatenated into a single string corpus. Stopwords are removed, word frequency is counted and the 100 most frequent words are generated when the code is run. Below this header material in the Python file are three categories: general, geographic and custom. These categories contains sections, which will ultimately become the tags: General: agriculture, animals, clothing, conflict, crime, culture, economy, education, environment, family, food and drink, happiness, hardship, health, history, indigenous, labor, migration, recreation, religion, technology Geographic (based loosely on migration statistics from the 1910 Idaho census[4]): basque, britain, canada, china, finland, france, germany, greece, idaho, india, ireland, italy, japan, mexico, norway, philippines, poland, portugal, scotland, spain, sweden Custom (example from our Rural Women’s History Project): Marriage and Divorce, Motherhood, Reproductive Rights Each section has a list of fifty associated words that the script is searching for within the concatenated text corpus. These were generated using Chatgpt with the following qualifications: Specifically related to the section topic (e.g., agriculture, animals) Exclude homographs (words that are spelled the same but have different meanings) Place names and how certain nationalities would refer to themselves for the geographic sections (e.g., “Finnish”, “Suomalainen”, “Finland”, “Suomi”, “Helsinki”, “Espoo”, “Tampere”, “Vantaa”, “Oulu”) Complete section of the migration tag within the Python file &#10042; These produced a total of 2,250 associated words across the 45 sections. The script then tallies these words and produces the output shown below: Example of Text Mining Tool Output for the Rural Women's History Project Future iterations of this template will modularize the General, Geographic and Custom sections for easier navigation instead of its current form as a single, expansive Python file. See Appendix 1 in the final section of this site for an excerpt of the current iteration of this code or the entire GitHub repository that is linked on the landing page to the site for this presentation. Primary Tag Sheet Once this text mining data is produced from the combined transcripts, it is entered into a “primary tag sheet” in Google Sheets, located in the same folder as the transcripts for student workers to edit. Using the Text to Columns function, tag names are split into column A and their associated words into column B. Excerpt of a formatted primary tags sheet, utilizing the Rural Women's History Projecrt text mining tool output Formatting The student workers then open their transcription sheet to make the following changes necessary for the Oral History as Data framework, including: Removing rows between dialogue Revising header semantics (e.g., Speaker Names to speakers, Start Time to timestamp) Adding brackets to timestamps Example of a Formatted Transcript Sheet Apps Script The student worker then opens the Apps Script extension on the transcription sheet and enters the code detailed in Appendix 2. Transcribers only need to make two adjustments: change the sheet name of the transcript they are editing on line 6, and the URL of their primary tag sheet on line 13, then save and run the code. This will automatically search the text for these associated words and fill in their tag name within the tag column of the transcript. &#10042; Copyediting This process is not intended to replace human transcribers, rather shifting the focus from manual tagging to copy editing, reducing heavy lifting and repetition. If a tag is inappropriate, the transcriber pastes it to the additions or subtractions column so these changes are preserved for future runs of the Apps Script and reflected in the final import to the OHD digital collection. Crucially, student workers are encouraged to modify tag names, add or remove associated words and create new categories based on trends they identify within the primary tag sheet. Once these changes are made, the Apps Script can be quickly re-run on all transcripts to implement these updates . Rather than simply asking student workers to transcribe recordings—work that offers little to highlight on a CV and can lead to burnout and high turnover—this process allows transcribers to engage in coding, create and modify tags, and see those changes reflected instantly through the Apps Script process. Other advantages include: All tags use a controlled vocabulary. Tagging is more accurate, detailed, and relevant, helping researchers quickly identify thematic connections. Tagging establishes a knowledge framework relevant to the collection that transcribers might lack in historical, scientific, or regional contexts key to the recordings."
    },
    { 
        "url": "/content/4_findings.html",
        "title": "Findings",
        "text": "Pre and Post Process Tagging Visualization One of my main concerns during the testing phase was whether transcribers would find the Apps Script coding element confusing or anxiety-inducing. This was not the case. By breaking down and repairing elements of the code during our weekly meetings, I was able to demystify the process and explain the purpose of each component. Additionally, I received excellent feedback from transcribers based on their experience, which led to many improved iterations. From what I can gather, this approach has generated a significant increase in both the volume, accuracy and detail of oral history transcription work. In addition to the tools discussed in this article, other factors contributing to this progress may include: A more dynamic, interactive workflow leading to greater transcriber productivity Less repetitive labeling and formatting work Supplementary documentation helping transcribers navigate the more technical aspects of the workflow In addition to helping us meet our department’s accessibility standards, this process enabled us to complete our first non-English oral history collection in the form of the Hispanic Oral History Project, an initiative from 1991 copyedited by student worker Daniel Olortegui Vargas. This work in progress will be using this material to enhance the OHD item-level interface, allowing listeners to toggle between English and Non-English transcriptions. This new feature in the open-source OHD framework aims to promote the digitization of more diverse oral history collections both within and beyond the institution. Regarding the limitations of data-driven, human-edited automated tagging, program managers must communicate that automated tags are only a starting point. Tags may be incorrectly applied, missing or need to be applied more broadly to transcripts. Even when these measures are taken, the amount of detail this process accrues is drastic and easily distinguishable in OHD’s tagging visualization in the image above. One could argue that the density of the data now makes it difficult for the researcher to navigate, especially on mobile devices and this continues to be part of the conversation as we refine these processes. &#10042; Conclusion In discussing grant funding for digital initiatives, a colleague pointed out that the time-intensive nature of oral history projects often leads to their neglect. As they put it: “Would you rather present ten oral histories or 500 photographs?” This quantity focused selection criteria ultimately results in an existential threat, resulting in the physical vulnerability of audio materials as they languish in the archives. Bicentennial and community oral history initiatives, rich in non-academic perspective, offer a uniquely biographical account of places and provide valuable contrast and context to the accepted historical record. By utilizing machine learning, Python, and JavaScript approaches, this process aims to make digitizing these resources more efficient and accessible, promoting their preservation and availability to the public."
    },
    { 
        "url": "/content/5_references_apendices.html",
        "title": null,
        "text": "Contents: References | Notes | Appendices | Appendix 1. Excerpt of Python Text Mining Tool | Appendix 2. Apps Script Example for Linking Transcript to Primary Tag Sheet | About the Author &#10042; References Link J. Why Racial Bias Still Haunts Speech-Recognition AI. Built In; 2020 [cited 2024 Jul 8]. Available from: https://builtin.com/artificial-intelligence/racial-bias-speech-recognition-systems Freund A. From.wav to.txt: why we still need transcripts in the digital age. Internet. 2024. [cited 2024 Jul 8]. Allen SE. Resisting the editorial ego: editing oral history. Oral Hist Rev. 1982;10(1):33-45. DOI 10.1093/ohr/10.1.33. [cited 2024 Jul 8]. Available from: https://www.tandfonline.com/doi/full/10.1093/ohr/10.1.33 Notes [1] Digital Collections, University of Idaho. University of Idaho Library Digital Initiatives; 2024 [cited 2024 Jul 8]. Available from: https://www.lib.uidaho.edu/digital/collections.html [2] Speech to Text in Premiere Pro FAQ. Adobe; [cited 2024 Jul 8]. Available from: https://helpx.adobe.com/content/help/en/premiere-pro/using/speech-to-text-faq.html [3] Matt Miller. “Lomax Whisper,” September 12, 2024. https://thisismattmiller.com/post/lomax-whisper/. [4] Department of Commerce and Labor, Bureau of Statistics. Thirteenth Census of the United States: 1910. Statistics for Idaho. Washington (DC): Government Printing Office; 1913. [cited 2024 Jul 8]. Available from: https://www2.census.gov/library/publications/decennial/1910/abstract/statistics-for-idaho.pdf Appendices Appendix 1. Excerpt of Python Text Mining Tool import pandas as pd import string from nltk.corpus import stopwords from collections import Counter import re from textblob import TextBlob Download NLTK stopwords data import nltk nltk.download('stopwords') Define preprocess_text function def preprocess_text(text): if isinstance(text, str): # Check if text is a string text = re.sub(r'\\b\\w{1,4}\\b', '', text) # Remove short words (length &lt;= 4) text = text.translate(str.maketrans('', '', string.punctuation)) text = text.lower() # Convert text to lowercase else: text = '' # Replace NaNs with an empty string return text Load stopwords for both Spanish and English stop_words_spanish = set(stopwords.words('spanish')) stop_words_english = set(stopwords.words('english')) Combine both sets of stopwords stop_words = stop_words_spanish.union(stop_words_english) import os Directory containing CSV files directory = 'C:\\\\Users\\\\aweymouth\\\\Documents\\\\Github\\\\transcript_mining_base\\\\CSV' List of CSV file names file_names = [ 'rwhp070.csv', 'rwhp075.csv', 'rwhp079.csv', 'rwhp083.csv', 'rwhp088.csv', 'rwhp109.csv', 'rwhp123.csv', 'rwhp174.csv', 'rwhp225.csv', 'rwhp261.csv', 'rwhp277.csv', 'rwhp277.csv', 'rwhp297.csv', 'rwhp320.csv', 'rwhp323.csv', 'rwhp378.csv', 'rwhp385.csv', 'rwhp410.csv', 'rwhp418.csv', 'rwhp420.csv', 'rwhp421.csv', 'rwhp422.csv', 'rwhp425.csv', 'rwhp426.csv', 'rwhp427.csv' ] Construct file paths using os.path.join() file_paths = [os.path.join(directory, file_name) for file_name in file_names] dfs = [pd.read_csv(file_path, encoding='utf-8') for file_path in file_paths] Concatenate text data from all dataframes into a single corpus corpus = '' for df in dfs: text_series = df['text'].fillna('').astype(str).str.lower().str.strip() # Extract and preprocess text column corpus += ' '.join(text_series) + ' ' # Concatenate preprocessed text with space delimiter Preprocess the entire corpus cleaned_corpus = preprocess_text(corpus) Remove stopwords from the corpus filtered_words = [word for word in cleaned_corpus.split() if word not in stop_words and len(word) &gt;= 5] Count the frequency of each word word_freq = Counter(filtered_words) Get top 100 most frequent distinctive words with occurrences top_distinctive_words = word_freq.most_common(100) === General Section === from collections import Counter import re def find_agriculture_terms(corpus): # Define a list of agriculture-related terms agriculture_terms = [\"harvest\", \"tractor\", \"acreage\", \"crop\", \"livestock\", \"farm field\", \"barn building\", \"ranch\", \"garden\", \"orchard\", \"dairy\", \"cattle\", \"poultry\", \"equipment\", \"fertilizer\", \"seed\", \"irrigation\", \"plow\", \"farmhand\", \"hoe\", \"shovel\", \"milking\", \"hay\", \"silage\", \"compost\", \"weeding\", \"crop rotation\", \"organic\", \"gmo\", \"sustainable\", \"farming\", \"rural\", \"homestead\", \"tilling\", \"wheat\", \"corn maize\", \"soybean\", \"potato\", \"apple fruit\", \"berry\", \"honey\", \"apiary\", \"pasture\", \"combine harvester\", \"trailer\", \"baler\", \"thresher\"] # Initialize a Counter to tally occurrences of agriculture-related terms agriculture_word_freq = Counter() # Tokenize the corpus to handle multi-word expressions tokens = re.findall(r'\\b\\w+\\b', corpus.lower()) # Iterate over each token in the corpus for word in tokens: # Check if the token is an agriculture-related term if word in agriculture_terms: agriculture_word_freq[word] += 1 # Return the top 50 most common agriculture-related terms return agriculture_word_freq.most_common(50) Call the function to find agriculture-related terms in the corpus top_agriculture_terms = find_agriculture_terms(corpus) Print the top 50 agriculture-related terms print(\"## agriculture\") for word, count in top_agriculture_terms: print(f\"{word.capitalize()}: {count}\") Appendix 2. Apps Script Example for Linking Transcript to Primary Tag Sheet function fillTags() { // Get the active spreadsheet var spreadsheet = SpreadsheetApp.getActiveSpreadsheet(); // Get the transcript sheet by name var transcriptSheet = spreadsheet.getSheetByName(\"Callison3\"); if (!transcriptSheet) { Logger.log(\"Transcript sheet not found\"); return; } // Set the header in cell E1 to \"tags\" transcriptSheet.getRange(\"E1\").setValue(\"tags\"); // Get the tags spreadsheet by URL var tagsSpreadsheet = SpreadsheetApp.openByUrl(\"https://docs.google.com/spreadsheets/d/1soOfgdAjik_TL8WX9dDV9BaFb1RQJc8_BVu7sBGnNUE/edit?gid=419710039#gid=419710039\"); if (!tagsSpreadsheet) { Logger.log(\"Tags spreadsheet not found\"); return; } // Get the tags sheet within the tags spreadsheet var tagsSheet = tagsSpreadsheet.getSheetByName(\"tags\"); if (!tagsSheet) { Logger.log(\"Tags sheet not found\"); return; } // Get the range of the transcript column var transcriptRange = transcriptSheet.getRange(\"D2:D\" + transcriptSheet.getLastRow()); var transcriptValues = transcriptRange.getValues(); // Get the range of example words and tags in the tags sheet var exampleWordsRange = tagsSheet.getRange(\"B2:B\" + tagsSheet.getLastRow()); var tagsRange = tagsSheet.getRange(\"A2:A\" + tagsSheet.getLastRow()); var exampleWordsValues = exampleWordsRange.getValues(); var tagsValues = tagsRange.getValues(); // Create a map of example words to tags var tagsMap = {}; for (var i = 0; i &lt; exampleWordsValues.length; i++) { var word = exampleWordsValues[i][0].toLowerCase(); var tag = tagsValues[i][0]; tagsMap[word] = tag; } // Initialize an array to store the tags for each transcript entry var transcriptTags = []; // Loop through each transcript entry for (var i = 0; i &lt; transcriptValues.length; i++) { var text = transcriptValues[i][0]; var uniqueTags = []; if (typeof text === 'string') { // Use regular expression to extract words and handle punctuation var words = text.match(/\\b\\w+['-]?\\w*|\\w+['-]?\\w*\\b/g); // Check each word in the transcript entry against the tags map if (words) { for (var j = 0; j &lt; words.length; j++) { var word = words[j].toLowerCase().replace(/[.,!?;:()]/g, ''); var singularWord = word.endsWith('s') ? word.slice(0, -1) : word; if (tagsMap.hasOwnProperty(word) &amp;&amp; !uniqueTags.includes(tagsMap[word])) { uniqueTags.push(tagsMap[word]); } else if (tagsMap.hasOwnProperty(singularWord) &amp;&amp; !uniqueTags.includes(tagsMap[singularWord])) { uniqueTags.push(tagsMap[singularWord]); } } } } // Add the determined tags to the array transcriptTags.push([uniqueTags.join(\";\")]); } // Get the range of the tags column in the transcript sheet, starting from E2 var tagsColumn = transcriptSheet.getRange(\"E2:E\" + (transcriptTags.length + 1)); // Set the values in the tags column to the determined tags tagsColumn.setValues(transcriptTags); } About the Author Andrew Weymouth is the Digital Initiatives Librarian at University of Idaho, specializing in static web design to curate the institution’s special collections and partner with faculty and students on fellowship projects. His work spans digital scholarship projects at the universities of Oregon and Washington and the Tacoma Northwest Room archives, including long form audio public history projects, architectural databases, oral history and network visualizations. He writes about labor, architecture, underrepresented communities and using digital methods to survey equity in archival collections."
    },
    { 
        "url": "/",
        "title": "Home",
        "text": "Slides Git Repository for Transcript Text Mining Tool Overview This presentation will provide a walkthrough of new processes for creating subject tags across complete oral history collections developed at the U of I Center for Digital Inquiry and Learning over the summer of 2024. It outlines a workflow that empowers student workers to run, modify, and expand these tags during the copyediting process. The goal is to produce richer, more accurate tagging, allowing researchers to more easily identify connections across collections. This presentation will provide a detailed description of the workflow, explore the challenges it addresses, share pedagogical experiences of transcribers, and examine the limitations of data-driven, human-edited automated tagging. Contents: Background Challenges Process Findings References and Appendices Content: CC BY-NC-ND 4.0 Andrew Weymouth 2024 (get source code). Theme: Variation on workshop-template-b by evanwill"
    }];
